{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12536117,"sourceType":"datasetVersion","datasetId":7828104}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Initiating wandb","metadata":{}},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"wandb\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.login(key=secret_value)\nwandb.init(\n    project=\"FullyConnectedLinearClassification\",\n    name=\"xlm-roberta-base-arabic\",\n)","metadata":{"trusted":true,"id":"Sy5M9-ytNplG","outputId":"eea7e799-30bc-4164-d3e2-777105a637cc"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.preprocessing import LabelEncoder\nimport huggingface_hub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load datasets\ntrain_df = pd.read_csv(\"/kaggle/input/arageneval-train-data/train_data_splitted_topics.csv\")\nvalid_df = pd.read_csv(\"/kaggle/input/arageneval-train-data/valid_data_cleaned_topics.csv\")\n\n# Optional: concatenate text + topics as input\ntrain_df['input'] = train_df['cleaned_text']\nvalid_df['input'] = valid_df['cleaned_text']\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ntrain_df['label'] = label_encoder.fit_transform(train_df['author'])\nvalid_df['label'] = label_encoder.transform(valid_df['author'])\nnum_labels = len(label_encoder.classes_)\n\ntrain_ds = Dataset.from_pandas(train_df[['input', 'label']])\nvalid_ds = Dataset.from_pandas(valid_df[['input', 'label']])\ndataset = DatasetDict({\"train\": train_ds, \"validation\": valid_ds})","metadata":{"trusted":true,"id":"HXmtpH8HNplJ"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport transformers\nimport platform\nimport sklearn\nimport numpy as np\n\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"Transformers version:\", transformers.__version__)\nprint(\"Python version:\", platform.python_version())\nprint(\"Scikit-learn version:\", sklearn.__version__)\nprint(\"Numpy version:\", np.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"hf_token\")\nhuggingface_hub.login(secret_value)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MODEL_NAME = \"Omartificial-Intelligence-Space/GATE-AraBert-v1\"\nMODEL_NAME = \"bhavikardeshna/xlm-roberta-base-arabic\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ndef tokenize(example):\n    return tokenizer(example['input'], truncation=True, padding='max_length', max_length=512)\n\ntokenized_dataset = dataset.map(tokenize, batched=True)","metadata":{"trusted":true,"id":"a7DCpk1oNplW","outputId":"dc89a4bb-cc01-470e-c6d0-8da7cba46447","colab":{"referenced_widgets":["e556e8f7105c47a08176f84d217f7ec5","74d1351ee1674375941c90fdea2ea549","407b1c8a444448b3b046aa50dcc22d10","4b667f374c9c407398ea29e12a9aa7ab","a044d2d22cbb4665b7c0632167c2408d","b5dfbdd1c51b4e10a7b00f4dab0e7454"]}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    \"bhavikardeshna/xlm-roberta-base-arabic\",\n    num_labels=num_labels,\n    trust_remote_code=True\n)","metadata":{"trusted":true,"id":"cQ5_6to4Nplg","outputId":"1d0f0a59-c840-4a0f-f04e-45191b58c3d5","colab":{"referenced_widgets":["96939d3d4dc24c66848cd6905ea22c21","1c9f07530ed04af586fe179138458b63"]}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom transformers import TrainingArguments, Trainer\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Balancing loss computation between classes\n\ntrain_labels = train_df['label'].values\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = torch.tensor(class_weights, dtype=torch.float)\nclass_weights\n\nclass CustomTrainer(Trainer):\n    def __init__(self, class_weights, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = class_weights.to(self.args.device)\n\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.get(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n\n        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n        loss = loss_fct(logits, labels)\n\n        return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true,"id":"25IQ2sHENpll"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./xlm-roberta-base-arabic-AuthId\",\n    eval_strategy=\"steps\",          # Change from \"epoch\" to \"steps\"\n    eval_steps=500,                 # Evaluate every 500 steps\n    save_strategy=\"steps\",               \n    save_steps=500,                 # Save model every 500 steps too\n    logging_strategy=\"steps\",            \n    logging_steps=500,              # Log to wandb every 500 steps\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=4,\n    \n    learning_rate=8e-5,\n    weight_decay=0.1,\n    warmup_ratio =0.1,\n    lr_scheduler_type=\"cosine\",\n    \n    load_best_model_at_end=True,\n    save_total_limit=2,\n    metric_for_best_model=\"f1_macro\",\n    report_to=\"wandb\"              # Change from \"none\" to \"wandb\"\n)\n\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    labels = p.label_ids\n\n    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n        labels, preds, average='macro', zero_division=0\n    )\n    accuracy = accuracy_score(labels, preds)\n\n    return {\n        'accuracy': accuracy,\n        'precision_macro': precision_macro,\n        'recall_macro': recall_macro,\n        'f1_macro': f1_macro\n    }","metadata":{"trusted":true,"id":"LbknJQEgNpln"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    tokenizer=tokenizer,  # this warning is safe to ignore\n    compute_metrics=compute_metrics,\n    class_weights=class_weights  # Pass computed weights here\n)\n\ntrainer.train()","metadata":{"trusted":true,"id":"lePQZcTXNplp","outputId":"59ca00d9-c418-4056-aaa4-1007e65a3e48"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Saving Model","metadata":{}},{"cell_type":"code","source":"trainer.save_model(\"./xlm-roberta-base-arabic-AuthId\")\ntokenizer.save_pretrained(\"./xlm-roberta-base-arabic-AuthId\")","metadata":{"trusted":true,"id":"s1-n_uJQNpls","outputId":"137bde78-cad6-4781-b036-4bdd78feb0c9"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"huggingface_hub.login(token=secret_value)\n\napi = huggingface_hub.HfApi()\napi.create_repo(repo_id=\"MuhammadHelmy/xlm-roberta-base-arabic-AuthId\", repo_type=\"model\", exist_ok=True)\n\napi.upload_folder(\n    folder_path=\"/kaggle/working/xlm-roberta-base-arabic-AuthId\",\n    repo_id=\"MuhammadHelmy/xlm-roberta-base-arabic-AuthId\",\n    repo_type=\"model\"\n)","metadata":{"trusted":true,"id":"9W-dhtwNNplw","outputId":"2fd84aa3-b383-4603-c4b8-2a8b8156d649","colab":{"referenced_widgets":["378ccbd022aa49c7904185681d7fab40","0757842528604e34a036f2dfa45935ec","49be3de5ee9540beb0f2c23da3c9c367","1405cc7dcd69477197fd07425310e101","b3e87490510247229a808727a4570d2e","38ec0ea724c1497c840acc75aaa89352","723136af2f61425389806bb523fb8210","77e05499d0284c0ca6f884b6afb53f0c","0a320fd95ea34216b173036ab5dee1b3","34fe0c11fd7b4a748dae6c9dd5664208","d715cdf462824933826c8bb2e61ce520","d31b61705f3c4c379ff88aa3affc8fe8","42e30c21ecbe422da7ada84b694b4838"]}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n# from transformers import Trainer\n# import pandas as pd\n# from datasets import Dataset, DatasetDict\n# from sklearn.preprocessing import LabelEncoder\n# import huggingface_hub\n# from transformers import AutoModelForSequenceClassification\n# from transformers import AutoTokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the saved model and tokenizer from Hugging Face Hub\n# model = AutoModelForSequenceClassification.from_pretrained(\"MuhammadHelmy/GATE-AraBert-AuthId\")\n# tokenizer = AutoTokenizer.from_pretrained(\"MuhammadHelmy/GATE-AraBert-AuthId\")\n# trainer = Trainer(model=model, tokenizer=tokenizer) # Initialize with just model and tokenizer for prediction","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def tokenize(example):\n#     return tokenizer(example['input'], truncation=True, padding='max_length', max_length=512)\n\n# tokenized_dataset = dataset.map(tokenize, batched=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = trainer.predict(tokenized_dataset[\"validation\"])\ny_true = preds.label_ids\ny_pred = preds.predictions.argmax(axis=-1)\n\nprint(classification_report(y_true, y_pred, target_names=label_encoder.classes_))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Inference","metadata":{}},{"cell_type":"code","source":"# Load the test data\ntest_df = pd.read_csv(\"/kaggle/input/arageneval-train-data/test_data_cleaned.csv\")\n\n# Optional: concatenate text + topics as input if you used this for training\ntest_df['input'] = test_df['cleaned_text']\n\n# Prepare the test dataset for prediction\ntest_ds = Dataset.from_pandas(test_df[['input']])\n\n# def tokenize(example):\n#     return tokenizer(example['input'], truncation=True, padding='max_length', max_length=512)\n\n# Tokenize the test dataset\ntokenized_test_dataset = test_ds.map(tokenize, batched=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make predictions on the test dataset\ntest_predictions = trainer.predict(tokenized_test_dataset)\n\n# Get predicted labels for the test set\ny_pred_test = np.argmax(test_predictions.predictions, axis=-1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Map back to original author names\ntest_df['label'] = label_encoder.inverse_transform(y_pred_test)\n\ntest_df = test_df[['id','label']]\n\n# Display the test dataframe with predictions\ndisplay(test_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.to_csv(\"predictions.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}